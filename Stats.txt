using random batch sampling
dropout used on fully connected layers
Batch size = 100
We can see significant overfitting of results 

0.504202 M parameters
step 0: train loss 2.3037, val loss 2.3038
step 500: train loss 0.1474, val loss 0.1459
step 1000: train loss 0.0747, val loss 0.0846
step 1500: train loss 0.0509, val loss 0.0637
step 2000: train loss 0.0378, val loss 0.0585
step 2500: train loss 0.0286, val loss 0.0509
step 3000: train loss 0.0252, val loss 0.0519
step 3500: train loss 0.0187, val loss 0.0519
step 4000: train loss 0.0196, val loss 0.0564
step 4500: train loss 0.0125, val loss 0.0575
step 5000: train loss 0.0124, val loss 0.0584
step 5500: train loss 0.0108, val loss 0.0542
step 6000: train loss 0.0095, val loss 0.0550
step 6500: train loss 0.0061, val loss 0.0533
step 7000: train loss 0.0052, val loss 0.0579
step 7500: train loss 0.0043, val loss 0.0521
step 8000: train loss 0.0034, val loss 0.0527
step 8500: train loss 0.0030, val loss 0.0496
step 9000: train loss 0.0049, val loss 0.0542
step 9500: train loss 0.0049, val loss 0.0661
step 10000: train loss 0.0027, val loss 0.0702
step 10500: train loss 0.0023, val loss 0.0538
step 11000: train loss 0.0024, val loss 0.0630
step 11500: train loss 0.0036, val loss 0.0650
step 12000: train loss 0.0021, val loss 0.0597
step 12500: train loss 0.0026, val loss 0.0634
step 13000: train loss 0.0016, val loss 0.0708
step 13500: train loss 0.0035, val loss 0.0765
step 14000: train loss 0.0057, val loss 0.0785
step 14500: train loss 0.0023, val loss 0.0709
step 14999: train loss 0.0011, val loss 0.0556

using random batch sampling
dropout used on fully connected layers, max pooling
Batch size = 100
0.504202 M parameters
significantly less overfitting and better results
step 0: train loss 2.3037, val loss 2.3038
step 500: train loss 0.2892, val loss 0.2719
step 1000: train loss 0.1223, val loss 0.1208
step 1500: train loss 0.0920, val loss 0.0835
step 2000: train loss 0.0708, val loss 0.0689
step 2500: train loss 0.0665, val loss 0.0580
step 3000: train loss 0.0620, val loss 0.0644
step 3500: train loss 0.0467, val loss 0.0506
step 4000: train loss 0.0487, val loss 0.0518
step 4500: train loss 0.0420, val loss 0.0524
step 5000: train loss 0.0440, val loss 0.0499
step 5500: train loss 0.0397, val loss 0.0447
step 6000: train loss 0.0367, val loss 0.0443
step 6500: train loss 0.0331, val loss 0.0423
step 7000: train loss 0.0286, val loss 0.0391
step 7500: train loss 0.0281, val loss 0.0424
step 8000: train loss 0.0253, val loss 0.0403
step 8500: train loss 0.0274, val loss 0.0368
step 9000: train loss 0.0234, val loss 0.0365
step 9500: train loss 0.0237, val loss 0.0398
step 10000: train loss 0.0210, val loss 0.0373
step 10500: train loss 0.0203, val loss 0.0371
step 11000: train loss 0.0193, val loss 0.0371
step 11500: train loss 0.0173, val loss 0.0354
step 12000: train loss 0.0216, val loss 0.0332
step 12500: train loss 0.0174, val loss 0.0350
step 13000: train loss 0.0186, val loss 0.0426
step 13500: train loss 0.0159, val loss 0.0350
step 14000: train loss 0.0143, val loss 0.0371
step 14500: train loss 0.0144, val loss 0.0351
step 14999: train loss 0.0136, val loss 0.0325

using random batch sampling
dropout used on fully connected layers, max pooling
max pooling dropout will be using dropout2d
Batch size = 100 
0.504202 M parameters
the sampling either gave more overfitting or the model reached optimal training quicker
results found are not better
step 0: train loss 2.3037, val loss 2.3038
step 500: train loss 0.2443, val loss 0.2311
step 1000: train loss 0.1159, val loss 0.1141
step 1500: train loss 0.0843, val loss 0.0768
step 2000: train loss 0.0655, val loss 0.0649
step 2500: train loss 0.0616, val loss 0.0560
step 3000: train loss 0.0534, val loss 0.0555
step 3500: train loss 0.0385, val loss 0.0504
step 4000: train loss 0.0415, val loss 0.0500
step 4500: train loss 0.0379, val loss 0.0537
step 5000: train loss 0.0352, val loss 0.0506
step 5500: train loss 0.0327, val loss 0.0459
step 6000: train loss 0.0318, val loss 0.0478
step 6500: train loss 0.0272, val loss 0.0408
step 7000: train loss 0.0228, val loss 0.0387
step 7500: train loss 0.0239, val loss 0.0425
step 8000: train loss 0.0194, val loss 0.0427
step 8500: train loss 0.0207, val loss 0.0401
step 9000: train loss 0.0180, val loss 0.0397
step 9500: train loss 0.0198, val loss 0.0464
step 10000: train loss 0.0157, val loss 0.0435
step 10500: train loss 0.0146, val loss 0.0374
step 11000: train loss 0.0146, val loss 0.0420
step 11500: train loss 0.0134, val loss 0.0400
step 12000: train loss 0.0145, val loss 0.0366
step 12500: train loss 0.0123, val loss 0.0396
step 13000: train loss 0.0136, val loss 0.0475
step 13500: train loss 0.0114, val loss 0.0410
step 14000: train loss 0.0102, val loss 0.0450
step 14500: train loss 0.0111, val loss 0.0425
step 14999: train loss 0.0083, val loss 0.0380

using random batch sampling
dropout used on fully connected layers, max pooling, cnn
max pooling dropout will be using dropout2d
Batch size = 100 
0.504202 M parameters
the sampling either gave more overfitting or the model reached optimal training quicker
0.504202 M parameters
step 0: train loss 2.3037, val loss 2.3038
step 500: train loss 0.5448, val loss 0.5211
step 1000: train loss 0.1855, val loss 0.1844
step 1500: train loss 0.1263, val loss 0.1125
step 2000: train loss 0.1022, val loss 0.0949
step 2500: train loss 0.0903, val loss 0.0800
step 3000: train loss 0.0805, val loss 0.0770
step 3500: train loss 0.0627, val loss 0.0673
step 4000: train loss 0.0640, val loss 0.0633
step 4500: train loss 0.0558, val loss 0.0620
step 5000: train loss 0.0593, val loss 0.0631
step 5500: train loss 0.0544, val loss 0.0546
step 6000: train loss 0.0502, val loss 0.0572
step 6500: train loss 0.0458, val loss 0.0478
step 7000: train loss 0.0405, val loss 0.0474
step 7500: train loss 0.0405, val loss 0.0503
step 8000: train loss 0.0360, val loss 0.0510
step 8500: train loss 0.0382, val loss 0.0439
step 9000: train loss 0.0338, val loss 0.0425
step 9500: train loss 0.0336, val loss 0.0497
step 10000: train loss 0.0301, val loss 0.0462
step 10500: train loss 0.0275, val loss 0.0417
step 11000: train loss 0.0284, val loss 0.0433
step 11500: train loss 0.0245, val loss 0.0414
step 12000: train loss 0.0298, val loss 0.0368
step 12500: train loss 0.0271, val loss 0.0414
step 13000: train loss 0.0276, val loss 0.0468
step 13500: train loss 0.0240, val loss 0.0411
step 14000: train loss 0.0224, val loss 0.0445
step 14500: train loss 0.0227, val loss 0.0435
step 14999: train loss 0.0206, val loss 0.0392

0.504202 M parameters
step 0: train loss 2.3037, val loss 2.3038
step 500: train loss 0.7319, val loss 0.6997
step 1000: train loss 0.2328, val loss 0.2334
step 1500: train loss 0.1596, val loss 0.1434
step 2000: train loss 0.1212, val loss 0.1146
step 2500: train loss 0.1132, val loss 0.1020
step 3000: train loss 0.1000, val loss 0.0986
step 3500: train loss 0.0879, val loss 0.0906
step 4000: train loss 0.0788, val loss 0.0811
step 4500: train loss 0.0769, val loss 0.0797
step 5000: train loss 0.0757, val loss 0.0754
step 5500: train loss 0.0729, val loss 0.0695
step 6000: train loss 0.0606, val loss 0.0638
step 6500: train loss 0.0607, val loss 0.0673
step 7000: train loss 0.0591, val loss 0.0630
step 7500: train loss 0.0585, val loss 0.0639
step 8000: train loss 0.0509, val loss 0.0600
step 8500: train loss 0.0479, val loss 0.0494
step 9000: train loss 0.0501, val loss 0.0560
step 9500: train loss 0.0489, val loss 0.0567
step 10000: train loss 0.0400, val loss 0.0516
step 10500: train loss 0.0484, val loss 0.0549
step 11000: train loss 0.0505, val loss 0.0590
step 11500: train loss 0.0418, val loss 0.0522
step 12000: train loss 0.0457, val loss 0.0478
step 12500: train loss 0.0422, val loss 0.0533
step 13000: train loss 0.0456, val loss 0.0615
step 13500: train loss 0.0373, val loss 0.0470
step 14000: train loss 0.0367, val loss 0.0546
step 14500: train loss 0.0392, val loss 0.0509
step 14999: train loss 0.0317, val loss 0.0444

32 batch size with dropout
0.504202 M parameters
step 0: train loss 2.3037, val loss 2.3043
step 500: train loss 1.2885, val loss 1.2665
step 1000: train loss 0.4457, val loss 0.3994
step 1500: train loss 0.2172, val loss 0.2268
step 2000: train loss 0.1684, val loss 0.1689
step 2500: train loss 0.1597, val loss 0.1486
step 3000: train loss 0.1270, val loss 0.1282
step 3500: train loss 0.1133, val loss 0.1043
step 4000: train loss 0.0886, val loss 0.0903
step 4500: train loss 0.0824, val loss 0.0881
step 5000: train loss 0.0837, val loss 0.0754
step 5500: train loss 0.0849, val loss 0.0798
step 6000: train loss 0.0726, val loss 0.0758
step 6500: train loss 0.0646, val loss 0.0687
step 7000: train loss 0.0688, val loss 0.0722
step 7500: train loss 0.0707, val loss 0.0670
step 8000: train loss 0.0703, val loss 0.0541
step 8500: train loss 0.0701, val loss 0.0707
step 9000: train loss 0.0672, val loss 0.0590
step 9500: train loss 0.0510, val loss 0.0637
step 10000: train loss 0.0559, val loss 0.0587
step 10500: train loss 0.0533, val loss 0.0566
step 11000: train loss 0.0531, val loss 0.0579
step 11500: train loss 0.0529, val loss 0.0515
step 12000: train loss 0.0508, val loss 0.0466
step 12500: train loss 0.0467, val loss 0.0489
step 13000: train loss 0.0525, val loss 0.0456
step 13500: train loss 0.0452, val loss 0.0505
step 14000: train loss 0.0420, val loss 0.0502
step 14500: train loss 0.0380, val loss 0.0554
step 14999: train loss 0.0445, val loss 0.0460

leave the initial values instead of doing a floor division
0.504202 M parameters
step 0: train loss 2.5753, val loss 2.5865
step 500: train loss 0.7051, val loss 0.6757
step 1000: train loss 0.1866, val loss 0.1722
step 1500: train loss 0.1192, val loss 0.0992
step 2000: train loss 0.0762, val loss 0.0742
step 2500: train loss 0.0635, val loss 0.0519
step 3000: train loss 0.0552, val loss 0.0473
step 3500: train loss 0.0505, val loss 0.0380
step 4000: train loss 0.0494, val loss 0.0375
step 4500: train loss 0.0477, val loss 0.0381
step 5000: train loss 0.0340, val loss 0.0338
step 5500: train loss 0.0320, val loss 0.0347
step 6000: train loss 0.0325, val loss 0.0318
step 6500: train loss 0.0344, val loss 0.0331
step 7000: train loss 0.0275, val loss 0.0290
step 7500: train loss 0.0268, val loss 0.0251
step 8000: train loss 0.0261, val loss 0.0292
step 8500: train loss 0.0255, val loss 0.0255
step 9000: train loss 0.0257, val loss 0.0249
step 9500: train loss 0.0229, val loss 0.0270
step 10000: train loss 0.0239, val loss 0.0262
step 10500: train loss 0.0164, val loss 0.0241
step 11000: train loss 0.0190, val loss 0.0222
step 11500: train loss 0.0181, val loss 0.0247
step 12000: train loss 0.0190, val loss 0.0207
step 12500: train loss 0.0149, val loss 0.0208
step 13000: train loss 0.0164, val loss 0.0216
step 13500: train loss 0.0175, val loss 0.0189
step 14000: train loss 0.0181, val loss 0.0193
step 14500: train loss 0.0180, val loss 0.0241
step 15000: train loss 0.0177, val loss 0.0224
step 15500: train loss 0.0180, val loss 0.0228
step 16000: train loss 0.0130, val loss 0.0209
step 16500: train loss 0.0117, val loss 0.0177
step 17000: train loss 0.0133, val loss 0.0201
step 17500: train loss 0.0148, val loss 0.0232
step 18000: train loss 0.0130, val loss 0.0198
step 18500: train loss 0.0139, val loss 0.0189
step 19000: train loss 0.0112, val loss 0.0181
step 19500: train loss 0.0114, val loss 0.0202
step 20000: train loss 0.0100, val loss 0.0178
step 20500: train loss 0.0124, val loss 0.0245
step 21000: train loss 0.0135, val loss 0.0223
step 21500: train loss 0.0116, val loss 0.0223
step 22000: train loss 0.0103, val loss 0.0227
step 22500: train loss 0.0112, val loss 0.0177
step 23000: train loss 0.0097, val loss 0.0171
step 23500: train loss 0.0073, val loss 0.0157
step 24000: train loss 0.0077, val loss 0.0180
step 24500: train loss 0.0065, val loss 0.0199
step 25000: train loss 0.0082, val loss 0.0179
step 25500: train loss 0.0098, val loss 0.0227
step 26000: train loss 0.0097, val loss 0.0186
step 26500: train loss 0.0120, val loss 0.0168
step 27000: train loss 0.0066, val loss 0.0181
step 27500: train loss 0.0068, val loss 0.0206
step 28000: train loss 0.0100, val loss 0.0206
step 28500: train loss 0.0065, val loss 0.0146
step 29000: train loss 0.0088, val loss 0.0185
step 29500: train loss 0.0081, val loss 0.0148
step 29999: train loss 0.0083, val loss 0.0178

regularizing by dividing by 255
0.504202 M parameters
step 0: train loss 2.3034, val loss 2.3037
step 2000: train loss 0.0775, val loss 0.0753
step 4000: train loss 0.0455, val loss 0.0451
step 6000: train loss 0.0380, val loss 0.0294
step 8000: train loss 0.0253, val loss 0.0286
step 10000: train loss 0.0210, val loss 0.0273
step 12000: train loss 0.0202, val loss 0.0278
step 14000: train loss 0.0169, val loss 0.0186
step 16000: train loss 0.0167, val loss 0.0210
step 18000: train loss 0.0117, val loss 0.0172
step 20000: train loss 0.0086, val loss 0.0155
step 22000: train loss 0.0124, val loss 0.0175
step 24000: train loss 0.0093, val loss 0.0180
step 26000: train loss 0.0079, val loss 0.0182
step 28000: train loss 0.0086, val loss 0.0122
step 29999: train loss 0.0062, val loss 0.0164

regularizing by dividing by 255, then transforming so the mean is 0.5 std is 0.5
0.504202 M parameters
step 0: train loss 2.3034, val loss 2.3038
step 2000: train loss 0.0788, val loss 0.0737
step 4000: train loss 0.0459, val loss 0.0449
step 6000: train loss 0.0381, val loss 0.0289
step 8000: train loss 0.0243, val loss 0.0271
step 10000: train loss 0.0206, val loss 0.0261
step 12000: train loss 0.0193, val loss 0.0270
step 14000: train loss 0.0167, val loss 0.0198
step 16000: train loss 0.0171, val loss 0.0208
step 18000: train loss 0.0122, val loss 0.0186
step 20000: train loss 0.0087, val loss 0.0150
step 22000: train loss 0.0124, val loss 0.0173
step 24000: train loss 0.0096, val loss 0.0176
step 26000: train loss 0.0080, val loss 0.0190
step 28000: train loss 0.0082, val loss 0.0121
step 29999: train loss 0.0059, val loss 0.0146

